{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2860c465-b905-4230-a182-e45f49a5621b",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 3. Recognizing \"Real\" vs. \"Fake\" jokes\n",
    "\n",
    "In [2.FakePunchlines](2.FakePunchlines.ipynb), we got GPT-2 to generate the punchlines to some jokes.  A human can pretty easily tell which are the real punchlines and which are the GPT-2-generated ones.  But can GPT-2 fool another AI?\n",
    "    \n",
    "In this Notebook, we'll use the HugginFace [transformers](https://github.com/huggingface/transformers) library to train an NLP classifier to distinguish between the real joke punchlines and the fake ones generated by GPT-2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3474abf-3593-47bf-bef6-7dddae41db88",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "We start by loading in our training and test datasets that were generated in [1.JokesDataset](1.JokesDataset.ipynb).  While we're trying to get things to work, let's downsample the data by 100x (i.e., use only 1% of the data), just so that things will run fast.  When we're ready to train for real, we'll use a factor of 1x (no downsampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be1b1dc-117a-4213-bf32-636f913d51b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our dataset of real and fake jokes, split into training and test sets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# For development purposes, we downsample by 100x, just so things run fast\n",
    "downsample_factor = 100\n",
    "# Load the data, filter out any non-string input (it breaks the code), and downsample\n",
    "dataset = load_dataset('csv', data_files={'train':['data/short_jokes_train.csv','data/short_jokes_train_fake.csv'],\n",
    "                                          'test':['data/short_jokes_test.csv','data/short_jokes_test_fake.csv']})\n",
    "dataset = dataset.filter(lambda ex,j: ((type(ex['setup'])==str) & (type(ex['punchline'])==str) & \n",
    "                                       (j%downsample_factor==0)),                         \n",
    "                         with_indices=True)\n",
    "print('{} rows in the train dataset ({}x downsampled).'.format(dataset['train'].num_rows,downsample_factor))\n",
    "print('{} rows in the test dataset ({}x downsampled).'.format(dataset['test'].num_rows,downsample_factor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71388c8f-d022-4aca-a001-3f8061ff4c9e",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "We've loaded the data as a HuggingFace \"dataset\", which can be read incrementally from disk (instead of loading the whole dataset into memory at once), and which can be fed nicely into PyTorch DataLoaders when it comes time to train a model.  \n",
    "\n",
    "Notice that we've mixed together the \"real\" jokes and the \"fake\" jokes (whose punchlines are generated by GPT-2) in both our \"train\" and \"test\" datasets.  The difference is that all the \"real\" jokes have score > 0 (because we only selected jokes with at least one upvote), whereas we created all the fake jokes with score = 0.\n",
    "   \n",
    "Let's take a quick look at how the data are formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16cee6-06f8-40a6-be97-7cb819f1369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d66d7-7de1-4988-a722-ea9c893ea0f0",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "The dataset is stored as a DatasetDict, which has two components, \"train\" and \"test\", each of which contain the data rows and the \"features\" from the CSV.\n",
    "    \n",
    "We can look at the first examples of the \"real\" and the \"fake\" jokes in our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521a00c-444c-48b4-906b-4f1a30f79f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A joke with a human-generated punchline:')\n",
    "print(dataset['test'][0])\n",
    "print()\n",
    "print('A joke with a GPT2-generated punchline:')\n",
    "print([x for x in dataset['test'] if x['score']==0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be53482b-ea00-4f19-8e5e-069884e6eb9f",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Now that we've loaded our data, let's use it to train a model!\n",
    "    \n",
    "We're not, however, going to start training from scratch.  \n",
    "    \n",
    "Transformer models, each with 100s of millions or even 100s of *billions* of free parameters trained on an enormous corpus of documents, are very expensive to train, in terms of both time and compute costs.  Not only do we not want to wait through and pay for all that compute time, but the carbon footprint of training a state-of-the-art model is [LARGE](https://huggingface.co/course/chapter1/4?fw=pt#transformers-are-big-models).  \n",
    "    \n",
    "Instead, we will use [transfer learning](https://towardsdatascience.com/cnn-transfer-learning-fine-tuning-9f3e7c5806b2).  To do that, we start from a pre-trained model, which has already been trained through many epochs on huge document datasets.  We will then do some \"fine-tuning\" training using our Jokes dataset to produce a classifier that is particularly good at distinguishing human-generated vs. GPT2-generated jokes.\n",
    "    \n",
    "Let's start with the [BERT model](https://arxiv.org/pdf/1810.04805.pdf) from Google's AI Language lab.  We're going to use it to do \"sequence classification\", where the model decides if one sequence (in our case, the punchline) is an appropriate follow-on from a previous sequence (in our case, the setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc012c9-ec2d-428c-82fc-bf6f2f561422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_tools as mtools\n",
    "\n",
    "checkpoint = mtools.load_checkpoint('bert')\n",
    "tokenizer = mtools.load_tokenizer(checkpoint)\n",
    "model = mtools.load_model(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647bcc09-6bc6-49ab-a05f-c34c15f785bd",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "We got some warnings that we've loaded a version of BERT that is not already set up for sequence classification, and that it needs some training to be ready to use this way.  That's okay!  Training is exactly what we are about to do.\n",
    "    \n",
    "But first, we need to tokenize the data using BERT's tokenizer, so that our text is encoded using the same token mapping that BERT has been pre-trained to expect.  \n",
    "    \n",
    "Two additional processes will happen as part of this tokenization: we'll pad the tokenized strings in each batch to be the same length, so that they can be loaded together into a single PyTorch tensor, and we'll generate the associated attention mask that tells the model which tokens are padding tokens that can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac068d4-45f2-48c3-950d-465abddd5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "\n",
    "import data_tools as dtools\n",
    "\n",
    "# Use a tokenize function to deal with tokenization and (batch) padding:\n",
    "#    -- all tokenized strings in a batch need to be padded to the same length \n",
    "#       to be loaded into a PyTorch tensor together\n",
    "def tokenize_function(example):\n",
    "    full_qa = dtools.joke_as_qa(example['setup'], example['punchline'])\n",
    "    q = [x[:x.find('Answer:')].strip() for x in full_qa]\n",
    "    a = [x[x.find('Answer:'):].strip() for x in full_qa]\n",
    "    return tokenizer(q, a, padding=\"max_length\", max_length=60, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e34e77-105d-47b6-8b85-a6ac5dfaf199",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Notice that the tokenization process has generated and added the token \"input_ids\", \"token_type_ids\", and \"attention_mask\" to our data structure.  These features are what gets input to the model training process.\n",
    "    \n",
    "We also need to add the classification labels that we use to distinguish \"real\" from \"fake\" jokes.  In our case, the \"real\" data all have *score>0*, while \"fake\" data have *score=0*, so we will map everything with *score>0* to have *label=1* and everything with *score=0* to have *label=0*.\n",
    "    \n",
    "Finally, we want to drop the input columns from the dataset, now that we've generated the token lists, attention masks, and labels that we will pass to the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88efba32-8799-4a93-ac78-dd0734de7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.map(lambda batch: {\"labels\": [int(x > 0) for x in batch[\"score\"]]}, batched=True)\n",
    "\n",
    "# Clean up / reformat data to fit into a PyTorch DataLoader\n",
    "# We don't need the text strings themselves anymore\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"setup\", \"punchline\", \"score\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc43dfa-be99-4dfe-8252-453adf92bb14",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Now we're ready to do the fine-tuning training of our classifier!  The training loop itself is implemented in the *train_classifier()* function defined in [model_tools.py](model_tools.py).\n",
    "    \n",
    "We'll train through just 3 epochs with our downsampled training set, just to see how well BERT does with minimal training.  The fake jokes looked pretty different from the real ones, so it shouldn't be too hard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1812b626-fe16-4411-a14d-c73a34d3d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mtools.train_classifier(tokenized_datasets, model, epochs=3)\n",
    "\n",
    "from datetime import datetime\n",
    "checkpoint_name = checkpoint.split('/')[-1].split('-')[0]\n",
    "filename = 'models/ClassifyJokes_{}_{:4.2f}subset_{}'.format(checkpoint_name,1.0/downsample_factor,datetime.now().date())+'.pt'\n",
    "\n",
    "from torch import save\n",
    "print('Saving model as {}'.format(filename))\n",
    "save(model,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068435df-de1b-49d1-8390-a1f88ed45d60",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "As expected, the classifier does pretty well with minimal training: better than 95%.  It may not even be able to improve much with the full training set.  It takes ~30 seconds/epoch to run on 1% of the full training set, so it should take a couple of hours to run 3 epochs with the full training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b45a0-bf08-4c63-b6a5-9be1f995404e",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "The steps here have been encapsulated in the function *classify_punchlines()*, implemented in [model_tools.py](model_tools.py), so you can run the entire process documented above with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d6b44-c694-46b0-85ab-94d802a3754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from punchline_classifier import train_punchline_classifier\n",
    "\n",
    "train_files = ['data/short_jokes_train.csv','data/short_jokes_train_fake.csv']\n",
    "test_files = ['data/short_jokes_test.csv','data/short_jokes_test_fake.csv']\n",
    "\n",
    "# Set downsample=1 or leave out to train on the full training set (it defaults to 1)\n",
    "model = train_punchline_classifier(train_files, test_files, downsample=20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc04387-6ab3-4998-a36d-551b5aa9550a",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Using the entire dataset to train the classifier takes a couple of hours and gives only marginal improvement (~98% accuracy).  If you want to run it, we recommend running it from the command line in a detached screen, as in [2.FakePunchlines](2.FakePunchlines.ipynb).\n",
    "\n",
    "* `$> screen -S train_class`\n",
    "* `$> python punchline_classifier.py --train data/short_jokes_train.csv,data/short_jokes_train_fake.csv --test data/short_jokes_test.csv,data/short_jokes_test_fake.csv`\n",
    "\n",
    "Then \"Ctl-a d\" to detach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1040dcc-020d-4fde-add2-00b6918a78ee",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "When the model is finished running, we can load it with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eead96-43ab-4f95-9f9a-58757bba95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import load\n",
    "model = load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b069b25-768f-406a-a944-6f374989e18d",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Let's take a look at how well the predictions do, and look at examples that the classifier got wrong to understand what it can do and what its limitations are.\n",
    "    \n",
    "We start by making predictions for our test data and comparing it to the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4bde7-144a-4154-9ba9-f6f387539a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mtools.classify_punchlines(tokenized_datasets['test'],model)\n",
    "labels = list(tokenized_datasets['test']['labels'].squeeze().numpy())\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = None   # don't truncate the column text\n",
    "df = pd.DataFrame()\n",
    "df['labels'] = labels\n",
    "df['pred'] = pred\n",
    "df['jokes'] = [dtools.joke_as_qa(x['setup'], x['punchline']) for x in dataset['test']]\n",
    "confusion_matrix = df.groupby(['labels','pred']).size().unstack(fill_value=0)\n",
    "print(confusion_matrix)\n",
    "print()\n",
    "print('{:>10d} real jokes that the classifier correctly predicted to be real'.format(confusion_matrix[1].iloc[1]))\n",
    "print('{:>10d} fake jokes that the classifier correctly predicted to be fake'.format(confusion_matrix[0].iloc[0]))\n",
    "print('{:>10d} real jokes that the classifier thought were fake'.format(confusion_matrix[0].iloc[1]))\n",
    "print('{:>10d} fake jokes that the classifier thought were real'.format(confusion_matrix[1].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474524d4-c946-4a73-9a11-fa612b12967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Here are the real jokes the classifier thought were fake:')\n",
    "df[(df['pred']==0) & (df['labels']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8ce7a-0769-4815-ab99-7cc61a41c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Here are the fake jokes the classifier thought were real:')\n",
    "df[(df['pred']==1) & (df['labels']==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6fcf70-2b05-4358-9596-ebed57003a66",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "The classifier was able to do a good job sorting out the real jokes from the ones with AI-generated punchlines.  That means pre-trained, straight-out-of-the-box GPT-2 is not able to fool BERT (or, if we're going to anthropomorphize the AIs, GPT-2 can't make BERT laugh!).  \n",
    "    \n",
    "Let's see if GPT-2 can do better once it has been fine-tuned on a set of real jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e727cb6-7bd2-48b2-ad29-77df53ab4176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
