{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf2cad74-b61f-4470-98d3-feb1500b3db4",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "# Can you teach an AI to tell jokes?\n",
    "\n",
    "In this tutorial we will explore what happens when we get an AI to tell jokes using Transformer models and PyTorch, how much we can improve the performance with fine-tuning, and some of the major limitations we encounter along the way.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95ca7d-b081-4ed8-bc87-bbdd45812d22",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "We will do the following:\n",
    "    \n",
    "* Create a jokes dataset\n",
    "* Use a pretrained Transformer model to generate punchlines\n",
    "* Train a \"joke classifier\" to distinguish real punchlines from the \"fake\" Transformer-generated ones\n",
    "* Fine-tune the Transformer model to improve its generated punchlines\n",
    "* Use the joke classifier to measure the improvement from fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40964901-780f-40e7-807a-fab829c5d06b",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 1. The Jokes Dataset\n",
    "    \n",
    "For this project, we'll use a large public dataset: [\"One Million Reddit Jokes\"](https://www.kaggle.com/pavellexyr/one-million-reddit-jokes).\n",
    "    \n",
    "Jokes can take many forms.  Here, we'll limit ourselves to jokes that include a \"setup\" question, followed by a short \"punchline\" answer (e.g.: \"Why did the chicken cross the road?  To get to the other side.\")  \n",
    "    \n",
    "With a few other requirements, we have the following selection criteria:\n",
    "    \n",
    "* Question/Answer format\n",
    "* Short punchline (20 words max)\n",
    "* No missing or deleted punchlines\n",
    "* At least one up-vote\n",
    "* Remove duplicates\n",
    "\n",
    "This leaves a final dataset of ~150,000 short \"Q/A\" format jokes, which we will split into \"train\" and \"test\" sets. We'll also write out a \"mini\" subset of train/test jokes for quick experimentation.  \n",
    "    \n",
    "Let's make our jokes dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cf0c2-722d-433a-b45c-28b027d452cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/opt/cloudburst/shared/nlp_puchlines/one-million-reddit-jokes.csv'\n",
    "\n",
    "from clean_data import clean_data\n",
    "clean_data(file=file_path)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06952ab7-401f-4ccc-a9e4-9214d4089b6a",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 2. \"Fake\" Punchlines, generated by an AI\n",
    "\n",
    "Let's see if an existing, pre-trained Transformer model can tell jokes.\n",
    "\n",
    "We'll use GPT-2, the freely-available forerunner of the recent GPT-3 text-generation model that has generated so much press. GPT-2 and GPT-3 are both *autoregressive* models, which are optimized for generating text.  They take a prompt and generate additional new text that \"continues\" the thread.\n",
    "\n",
    "We'll format our jokes as:\n",
    "    \n",
    "> \"Question: [joke setup, ends in '?'] Answer: [joke punchline]\"\n",
    "    \n",
    "We load the GPT-2 model and its associated tokenizer (which converts text into numeric model input) and pass it a \"prompt\", in the form \n",
    "    \n",
    "> \"Question: [joke setup] Answer:\" \n",
    "    \n",
    "GPT-2 will then generate the continuing text that should come after \"Answer:\".  \n",
    "    \n",
    "The code checks to see if a GPU is available and uses it.  The GPU gets a 6.5x speedup over the CPU in our tests, but that still means it will take hours to generate fake punchlines for our full dataset of 140,000+ jokes.\n",
    "\n",
    "Let's do a small run with our \"minitest\" dataset of 300 jokes (split 70%/30% for training/testing) and take a look (takes ~2 minutes with GPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294be73-fa4f-464e-92ef-a55bbe660225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fake_punchlines import add_fake_punchlines\n",
    "add_fake_punchlines('data/short_jokes_minitrain.csv')\n",
    "add_fake_punchlines('data/short_jokes_minitest.csv')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de3dd7-c867-4f90-9028-ec6ab0e66e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at exampes of the real jokes and the fake punchlines we just created\n",
    "import pandas as pd\n",
    "mini_jokes = pd.read_csv('data/short_jokes_minitest.csv')\n",
    "mini_fakes = pd.read_csv('data/short_jokes_minitest_fake.csv')\n",
    "for i in [0,1,4]:\n",
    "    print('      Question: \"{}\"'.format(mini_jokes.iloc[i]['setup']))\n",
    "    print('Real Punchline: \"{}\"'.format(mini_jokes.iloc[i]['punchline']))\n",
    "    print('GPT2 Punchline: \"{}\"'.format(mini_fakes.iloc[i]['punchline']))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5eb029-3afa-4030-b987-9d08bedbab9d",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "    \n",
    "A few interesting things to note:\n",
    "\n",
    "* Responses are on-topic and sound (mostly) like coherent English.  This is what GPT-2 is good at!\n",
    "* Responses ramble on and cut off arbitrarily.  We set a 30-token limit if no end-of-string (EOS) token is received; an EOS token is basically *never* generated.  GPT-2 is not good at knowing when to shut up!\n",
    "* GPT-2 often answers questions with more questions (although structuring our prompts with explicit \"Question:/Answer:\" format seems to have helped a lot compared to my previous tests...)\n",
    "* Generating punchlines takes ~30 seconds per 100 jokes, even with the GPU.\n",
    "\n",
    "For now, let's stick with our quick 300-joke training set for illustration purposes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb9f90-22b1-491e-9ea7-c9911a9d0397",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 3. A Classifier to recognize \"real\" and \"fake\" punchlines\n",
    "    \n",
    "GPT-2 straight out of the box has trouble telling jokes: she tends to ramble on and on, she often answers questions with more questions, and her jokes aren't very funny.  \n",
    "\n",
    "She can't fool a human into thinking that she's a real, human comedian.  But can GPT-2 fool another AI?\n",
    "    \n",
    "Let's find out, by training a classifier to distinguish \"real\" from \"fake\" jokes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf122a2-713c-4242-b629-7da69b50a314",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "For this exercise, we'll use a different type of Transformer model: an *autoencoding* model called BERT.  The process of training and testing BERT is implemented in the *train_punchline_classifier()* function in [punchline_classifier.py](punchline_classifier.py).  \n",
    "    \n",
    "We need to train BERT on a joke dataset that includes both real jokes and fake jokes.  We also need a test set of jokes to quantify how well he does.  Here, we'll just use the small 300-joke dataset we created earlier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca387a-d845-4389-9a48-c4329ddecc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from punchline_classifier import train_punchline_classifier\n",
    "\n",
    "# 210 \"real\" and 210 \"fake\" jokes in our training set\n",
    "train_files = ['data/short_jokes_minitrain.csv','data/short_jokes_minitrain_fake.csv']\n",
    "# 90 \"real\" and 90 \"fake\" jokes in our test set\n",
    "test_files = ['data/short_jokes_minitest.csv','data/short_jokes_minitest_fake.csv']\n",
    "\n",
    "model = train_punchline_classifier(train_files, test_files)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efce87-61fe-41f0-b6e3-dcb6a7c6b4ad",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Notice that BERT is able to achieve 95%+ accuracy, even just training on our \"mini\" dataset of a few hundred jokes.\n",
    "    \n",
    "So the answer is \"No\", out-of-the-box GPT-2 cannot fool BERT with her joke-telling abilities.\n",
    "    \n",
    "But what if we train her *specifically to tell jokes*?  Can she get better at it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5964ec-ac69-43ce-b985-601aa5a2865f",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 4. Fine-Tuning a Text Generator \n",
    "    \n",
    "Next, we'll use our training set of short Q/A-style jokes to do some \"fine-tuning\" on the GPT-2 generator model.  This process makes use of the pre-trained GPT-2 ability to generate realistic English language text, but then trains a few more neural network layers to specifically generate the kind of text we're looking for.  \n",
    "    \n",
    "This fine-tuning model training is implemented in [fine_tune.py](fine_tune.py).  \n",
    "    \n",
    "If we only use our \"minitrain\" set, we don't have enough to get improvement from fine-tuning, so instead we'll use our full training set, downsampled by a factor of 10x.  ***This runs in about 15 minutes on a GPU*** and shows significant improvement over un-tuned models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e8450-9176-44e2-9db0-c341af87b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tune import fine_tune\n",
    "\n",
    "fine_tune(train_files='data/short_jokes_train.csv',\n",
    "          use_model='gpt2', downsample=10, nepochs=3)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b7284-cced-4f45-a03b-50418d9e67b3",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 5. How well does our fine-tuned Joke Generator perform?\n",
    "    \n",
    "Okay, so now we've done a round of fine-tuning on the joke generator.  Let's see if it performs any better than vanilla out-of-the-box GPT-2.  We'll do this two ways: \n",
    "    \n",
    "- By passing our AI-generated jokes to the Punchline Classifier to see if it can fool the classifier\n",
    "- By playing around with the joke-generator to see if it can make us laugh!\n",
    "    \n",
    "The following cell uses both out-of-the-box GPT-2 and our fine-tuned version to generate jokes, then runs the real jokes, the GPT-2 fake jokes, and the fine-tuned fake jokes through the BERT-based classifier we trained earlier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9b64c-91b6-4bf1-8893-faef5a4e6870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "mini_jokes = pd.read_csv('data/short_jokes_minitest.csv')\n",
    "mini_fakes = pd.read_csv('data/short_jokes_minitest_fake.csv')\n",
    "\n",
    "from test_generator import load_all_models, generate_punchlines, get_class_predictions\n",
    "\n",
    "# These are the models we just trained earlier.\n",
    "generator_filename = 'models/JokeGen_gpt2.pt'\n",
    "classifier_filename = 'models/ClassifyJokes_bert.pt'\n",
    "\n",
    "# Load the models\n",
    "load_all_models(generator_filename=generator_filename, classifier_filename=classifier_filename)\n",
    "# Generate punchlines using vanilla GPT-2 and our fine-tuned version\n",
    "generated_gpt2, generated_ft = generate_punchlines(mini_jokes)\n",
    "# Get predictions (1=\"real\" joke, 0=\"fake\" joke)\n",
    "p_human, p_gpt2, p_ft = get_class_predictions(mini_jokes, generated_gpt2, generated_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d7245-7295-45a3-b915-56ac7db938af",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Almost all (>97%) human-generated jokes are recognized as being \"real\" jokes. In contrast, almost none of the jokes generated by out-of-the-box GPT-2 can convince the classifier they are \"real\".\n",
    "\n",
    "The fine-tuned model does better, convincing the classifier that it has produced a \"real\" joke about 25-30% of the time.  This increases to ~40% if you take the time to fine-tune with the full training dataset over more epochs (recall we only trained with 1/10th of the training set for 3 epochs here)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc240e60-637b-4044-9d98-86a8767b3dc4",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Both out-of-the-box GPT-2 and our Fine-tuned model have a tendency to produce multiple \"Question:... Answer:...\" sequences in the punchline.  For example, vanilla GPT-2 produced this punchline in one of our runs:\n",
    "\n",
    "- Question: *\"Did you know Google now has a platform for recording your bowel movements?\"*\n",
    "- Answer: *\"Google+ Question: Does the internet only allow you to search for words on the internet? Answer: Yes. Answer: The internet\"*\n",
    "    \n",
    "This would make more sense if we truncated the punchline after its first \"Answer\", before it starts asking more questions and supplying more answers, i.e.:\n",
    "    \n",
    "- Question: *\"Did you know Google now has a platform for recording your bowel movements?\"*\n",
    "- Answer: *\"Google+\"*\n",
    "    \n",
    "If we \"cheat\", we can clean up the generated output by truncating it before any redundant \"Question:\" or \"Answer:\" sequences, can we do a better job of fooling the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea795bd9-3129-4e0c-b95c-eb247dd76c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_extras(text):\n",
    "    text = text.replace('\\n','')\n",
    "    while text.count('Question:') > 1:\n",
    "        text = text[:text.rfind('Question:')]\n",
    "    while text.count('Answer:') > 1:\n",
    "        text = text[:text.rfind('Answer:')]\n",
    "    return text\n",
    "\n",
    "p_human, p_gpt2, p_ft = get_class_predictions(mini_jokes, \n",
    "                                              [strip_extras(x) for x in generated_gpt2], \n",
    "                                              [strip_extras(x) for x in generated_ft])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a5ae7-aeb8-4909-86f9-8ae9b6bf54f5",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "That clearly helped!  Now >50% of the punchlines generated by the fine-tuned model can fool the classifier (while vanilla GPT-2 is still struggling at a below 10% hit rate).  \n",
    "    \n",
    "Now let's bundle up the fine-tuned generator to tell a joke.  We'll do the cleaning process on the result, and then we'll pass it through the classifier.  If the classifier thinks it's a real joke, we'll display the results.  Otherwise, we'll generate a new punchline and keep trying until we get one that can fool the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56a81a-9495-44ca-880c-7e0ab91959d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_generator import tell_a_joke\n",
    "\n",
    "setup = \"How many nerds does it take to change a lightbulb?\"\n",
    "\n",
    "joke = tell_a_joke(setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ffee1-40e2-487f-b2cd-ff6e55aa9abb",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Try running the same joke setup several times!  Usually, the joke generator will come up with something that is approved by the classifier within 1-3 attempts.  \n",
    "    \n",
    "Now try some different joke setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df48a9e-ff8c-4782-a9f2-ea7d6a70aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = \"Why did the chicken cross the road?\"\n",
    "\n",
    "joke = tell_a_joke(setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4db0ec-09b3-4e9e-99c9-89672508a02f",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "It isn't always funny, but at least these punchlines sound like they could be punchlines.  It's almost like your 5-year-old kid is coming up with them..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d751d-9613-423f-bda5-7181ff7ac9e1",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Finally, let's make sure to leave the campsite clean, since it's a shared resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed34adc-f918-4f68-824f-a01d7086c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exit Python to release GPU and restart kernel:\n",
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6fc4fd-9467-49f6-b816-bfc819897501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
