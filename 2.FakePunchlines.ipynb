{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f72eeac3-cb79-49fe-ad6a-fffcb5f67801",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "## 2. Generating Punchlines with a Pre-Trained Transformer Model\n",
    "\n",
    "What happens when we ask an AI to come up with the punchlines for jokes?\n",
    "\n",
    "In this exercise, we'll use a pre-trained transformer model, GPT-2, which is the freely-available forerunner of the recent GPT-3 text-generation model that generated tons of press last year. GPT-2 and GPT-3 are built to take a text prompt, and then generate additional new text that \"continues\" the thread.\n",
    "\n",
    "Given a joke setup, can GPT-2 produce a plausible punchline? And is it funny?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14ea4e-a341-4671-afe4-e0b26b465740",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Let's start by reading in the \"mini-test\" examples from our cleaned-up set of short Q/A-format jokes we assembled in the [JokesDataset Notebook](1_JokesDataset.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771480f0-fff6-43a5-94a8-2288d1e3f31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 jokes in the dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setup</th>\n",
       "      <th>punchline</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did you know Google now has a platform for rec...</td>\n",
       "      <td>It's called Google Sheets.</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What do you call a boat full of dentists?</td>\n",
       "      <td>A tooth ferry</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do you know someone is feeling horny?</td>\n",
       "      <td>They click on this post</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               setup  \\\n",
       "0  Did you know Google now has a platform for rec...   \n",
       "1          What do you call a boat full of dentists?   \n",
       "2          How do you know someone is feeling horny?   \n",
       "\n",
       "                    punchline  score  \n",
       "0  It's called Google Sheets.      9  \n",
       "1               A tooth ferry    126  \n",
       "2     They click on this post      2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_mini = pd.read_csv('data/short_jokes_minitest.csv',\n",
    "                      dtype={'setup':str,'punchline':str,'score':int},\n",
    "                      keep_default_na=False)\n",
    "print('{} jokes in the dataset'.format(df_mini.shape[0]))\n",
    "df_mini.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b98fe23-4c56-4b84-9121-5dc5835929b8",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "GPT-2 is trained to be general-purpose text generator, not necessarily to answer questions or provide punchlines.  We're therefore going to give it a few in-text clues that may help it recognize the Q/A joke format we are trying to produce.\n",
    "    \n",
    "Let's reformat each \"setup\" + \"punchline\" as a single text blob, with the format:\n",
    "    \n",
    "> \"Question: [setup text, ends with '?'] Answer: [punchline text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea0618fd-4827-4972-a8c1-dbfc0b1d1c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Did you know Google now has a platform for recording your bowel movements? Answer: It's called Google Sheets.\n",
      "Question: What do you call a boat full of dentists? Answer: A tooth ferry\n",
      "Question: How do you know someone is feeling horny? Answer: They click on this post\n"
     ]
    }
   ],
   "source": [
    "import data_tools as dtools\n",
    "\n",
    "df_mini['full_qa'] = df_mini.apply(lambda x: dtools.joke_as_qa(x['setup'], x['punchline'])[0], axis=1)\n",
    "for i in range(3): \n",
    "    print(df_mini.iloc[i]['full_qa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6076f02-2d85-4a9e-afa4-1ecaa5a2b6a2",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "This is the full joke text.  We'll use these full jokes as training data in the [FineTune Notebook](4_FineTune.ipynb) when we try to train GPT-2 to be better at generating punchlines.  \n",
    "    \n",
    "For now, using \"out-of-the-box\" GPT-2 to generate punchlines, we will provide it a prompt up through \"Answer:\" and let it fill in the answer.  The prompts look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e58b647c-8bb1-4e96-8876-030dd9205df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Did you know Google now has a platform for recording your bowel movements? Answer:\n",
      "Question: What do you call a boat full of dentists? Answer:\n",
      "Question: How do you know someone is feeling horny? Answer:\n"
     ]
    }
   ],
   "source": [
    "df_mini['prompt'] = df_mini['full_qa'].apply(lambda x: x[:x.find('Answer: ')+len('Answer:')])\n",
    "for i in range(3): \n",
    "    print(df_mini.iloc[i]['prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a0ca9-bde5-40b2-b1ed-0781dea98801",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "Now that we have our prompts ready to go, we need to load our GPT-2 model.  \n",
    "    \n",
    "I've written a wrapper that can load several different pre-trained models.  Each model consists of the following:\n",
    "\n",
    "* A model architecture - This is the network structure of transformer modules.\n",
    "\n",
    "* A checkpoink - This is a specific trained instance of the model architecture, along with all the associated weights.  A model architecture and checkpoint together specify everything you need to know to reconstruct a particular deep learning network.\n",
    "    \n",
    "* A tokenizer - Each transformer model has an associated tokenizer that is used to turn text strings into numeric tokens that represent the words (including punctuation and word-parts).  The numeric tokens are what get fed into the Transformer Model.  When you encode your text into tokens, it is critically important to use the *same* tokenizer the model was trained with.\n",
    "    \n",
    "We need to load a specific checkpoint, the ready-to-use model (architecture + weights) that it describes, and the tokenizer used to encode the data that model was trained on.  I'm doing this using the [HuggingFace](https://huggingface.co) `transformers` library and associated models, which are all open-source Python.  \n",
    "    \n",
    "*I highly recommend their [Transformers self-paced online course](https://huggingface.co/course/chapter1/1), if you'd like to learn more about using Transformer Models!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4714cbfb-ab57-4cc0-a47f-af4d2aad9087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint \"gpt2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "import model_tools as mtools\n",
    "\n",
    "checkpoint, tokenizer, model = mtools.load_model('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e38a1-1a3d-4fb9-9a93-f51f6c9896ad",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "We're now ready to use the pre-trained GPT-2 model and its associated tokenizer to generate joke punchlines from our prompt! \n",
    "    \n",
    "The text generator here is implemented in Pytorch and set to run on the GPU by default.  We just need to pass it the model, the tokenizer, and a list of prompts.  Let's start by just running it on some prompts from our \"minitest\" set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92d735e3-04c3-4d35-914c-db08e793172c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:08<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "output = mtools.generate(model, tokenizer, list(df_mini['prompt'])[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c1851-fa85-49e2-b47b-b538982ca5eb",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "If the GPU is available, it will use it. If no GPU is available, the generator will fall back on the CPU, which takes 6.5x longer (on the current server).  You can also force the generator to use the CPU with the keyword `use_gpu=False`.  (Note that, while the GPU was able to do multiple iterations / second, the CPU takes > 1 second / iteration!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30da2499-b736-4d02-8f1a-1f1ed9a02c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:52<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "output = mtools.generate(model, tokenizer, list(df_mini['prompt'])[:30], use_gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c3696-d3e1-45a2-b9db-98240a687d56",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "The generator does probabilistic text generation to find likely candidates for the \"next token\", and then choses randomly from a multinomial distribution, so every time you run it on a prompt, you will get different output.  \n",
    "    \n",
    "Let's strip the input prompts off the generated text and remove newlines (like we did for the original jokes), then look at some of the output we just generated, compared to the original punchline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a583c2f2-2d9a-4d5a-9ea8-ddadb6d8c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [x[x.find(df_mini.iloc[j]['prompt'])+len(df_mini.iloc[j]['prompt']):] for j,x in enumerate(output)]\n",
    "output = [x.replace('\\n',' ').replace('\\r',' ') for x in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "213253bb-18cc-47bb-9fad-f2738e5f6b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Question:  Did you know Google now has a platform for recording your bowel movements?\n",
      "      Answer:  It's called Google Sheets.\n",
      "GPT-2 Answer:  No.  Google recently updated their Privacy Policy to recognize that recording bowel movements is the number one priority for users of the service. Why should you\n",
      "---\n",
      "    Question:  What do you call a boat full of dentists?\n",
      "      Answer:  A tooth ferry\n",
      "GPT-2 Answer:  A man or woman with three nails.  Answer: A horse's nose.  Question: Why do you call a car a tractor,\n",
      "---\n",
      "    Question:  How do you know someone is feeling horny?\n",
      "      Answer:  They click on this post\n",
      "GPT-2 Answer:  Maybe you just have a bad day, you've been getting naked too much and are not interested in talking about the rest of the day. Don't\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('    Question:  {}'.format(df_mini.iloc[i]['setup'].strip()))\n",
    "    print('      Answer:  {}'.format(df_mini.iloc[i]['punchline'].strip()))\n",
    "    print('GPT-2 Answer:  {}'.format(output[i].strip()))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e7abc-dc31-4e2e-872a-d3e77ad52e83",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "    \n",
    "There are a few interesting things to note here.\n",
    "\n",
    "* Responses are generally on-topic and sound (mostly) like coherent English.  This is what GPT-2 is good at!\n",
    "* The responses just ramble on and cut off arbitrarily  We set a 30-token limit if no end-of-string (EOS) token is received; an EOS token is basically *never* generated.  GPT-2 is not good at knowing when to shut up!\n",
    "* GPT-2 often answers questions with more questions (although structuring our prompts with explicit \"Question:/Answer:\" format seems to have helped a lot compared to my previous tests...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a707ed-b845-4902-9dcc-a649aaadedab",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "In the next notebook, we'll use a different kind of Transformer model to train a classifier to differentiate between \"real\" jokes from the Reddit thread.  To do that, we'll need a nice big training set of \"real\" and \"fake\" jokes.  \n",
    "    \n",
    "We've got the real ones.  Now we need the fake ones.  \n",
    "    \n",
    "We'll make them by generating out-of-the-box GPT-2 punchlines.  That means we need to run our generator on the training and test datasets we created for our jokes dataset.  \n",
    "    \n",
    "All of the steps we performed above in this Notebook are packaged up in the `add_fake_punchlines()` function in `fake_punchlines.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226fe0c6-390e-41b1-9ca6-d8aa1c8ec918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 jokes in the dataset\n",
      "Using checkpoint \"gpt2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on jokes 0:100 out of 300 -- 0:00:00.000005\n",
      "Running on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing 0:100 to CSV\n",
      "Working on jokes 100:200 out of 300 -- 0:00:28.979446\n",
      "Running on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing 100:200 to CSV\n",
      "Working on jokes 200:300 out of 300 -- 0:00:57.913016\n",
      "Running on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:37<00:00,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing 200:300 to CSV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This runs locally in the Notebook on our small 300-joke \"mini-test\" set.  \n",
    "\n",
    "from fake_punchlines import add_fake_punchlines\n",
    "\n",
    "add_fake_punchlines('data/short_jokes_minitest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2467e5-0cef-4996-8d40-7b588433cbf9",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "As we saw, even with a GPU, it takes ~0.3 seconds to generate a punchline.  This means generating punchlines for all 140,000+ jokes in our training + test datasets will take almost 12 hours.  \n",
    "    \n",
    "If you have a stable internet connection and can leave your laptop open, you can run them right here in the notebook and hope that you don't get disconnected.  \n",
    "    \n",
    "However, a better choice is to run them from the terminal, using `screen` or `tmux` to background the process.  That way, you launch the run, close your laptop, and walk away.  The process will run overnight and the output will be waiting for you when you get up in the morning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51309d21-b64b-470d-8b68-f8bb3a2b4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next two lines will run on the entire training and test set, if you want to do that from the Notebook.\n",
    "# However, we recommend running those in the background at the command line, as described below.\n",
    "\n",
    "# add_fake_punchlines('data/short_jokes_test.csv')\n",
    "# add_fake_punchlines('data/short_jokes_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879a730-b3d3-41a5-a3b5-0ee1cb5ca8b9",
   "metadata": {},
   "source": [
    "<div style=background-color:#EEEEFF>\n",
    "\n",
    "To run in the background, do the following:\n",
    "* Select **File&rarr;New&rarr;Terminal** to open a Terminal window in a new tab.\n",
    "* Optional: drag the terminal tab to occupy the lower half of your browser window for a split-screen interface.\n",
    "* Issue the following commands:\n",
    "```\n",
    "    cd ~/examples/nlp_punchlines\n",
    "    screen -S fake_train\n",
    "    python fake_punchlines.py data/short_jokes_train.csv\n",
    "```\n",
    "* Then type \"Ctl-a d\" to detach from the screen.  The process will continue running in the background.\n",
    "    \n",
    "You can check on your background run by either:\n",
    "* Reattaching to the screen with \n",
    "```\n",
    "    screen -r fake_train\n",
    "```\n",
    "* Looking at the fake punchlines that are being written (in batches of 100) to `data/short_jokes_train_fake.csv` with\n",
    "```\n",
    "    tail data/short_jokes_train_fake.csv\n",
    "``` \n",
    "* If you reattached to check on your run, make sure to detach again with \"Ctl-a d\" before you walk away from your laptop!\n",
    "    \n",
    "Now set the test data running in the background in another screen:\n",
    "```\n",
    "    screen -S fake_test\n",
    "    python fake_punchlines.py data/short_jokes_test.csv\n",
    "```\n",
    "Remember to type \"Ctl-a d\" to detach from the screen!  The process will continue running in the background.\n",
    "\n",
    "With both processes running on the GPU, things will run a little slower on each thread (2.5 it/s instead of 3.5 it/s in our tests), but running both threads at the same time will effectively get you 5 it/s, so the whole process should complete in 6 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa11e48-c685-4a22-8696-9b455b37e278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
